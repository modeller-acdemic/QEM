Breakdown of each Python script and where it stores its respective data:

# 'qiskit_circuit_generator.py': 
Generates the random circuits with depths 2-9 and with a random length between 2-5, saving the circuits as circuits.csv to Data/training_data/circuits/. It also verifies the distribution with a graph, saved as length_distribution.png in 'Data/training_data/verification_figs/'.

# 'data_collector.py': 
A general data collection script. For any given fraction/proportion (from 0 to 1), it will load in the circuits.csv file, randomly assign the circuits to either the simulated or real hardware categories, and save a backup JSON file of the split ('partition.json'). This is so that, if the hardware retrieval fails due to exceeding available computation time on IBM QPUs (not uncommon), it allows you to batch your data, which will then be saved in a cached temp folder ('Data/training_data/cached_data/temp_{fraction}'). Then, the remaining batches can correctly continue where it left off when QPU access becomes available. It then retrieves the computations for the respectively assigned circuits from either the ibm_torino or the fake_torino backend, as assigned. The noise-free target observables are retrieved from AerSimulator. Once it has retrieved the data, it applies feature engineering method, and saves the data to the 'Data/training_data/feature_engineered/' sub-folder, along with its noise-free target data. It also saves the raw data, with no feature engineering, to the 'Data/training_data/raw_data/' subfolder. Finally, for verification purposes, it prints a distribution of gate counts and saves both a parametrised and non-parametrised gate distribution bar chart to the 'Data/training_data/verification_figs/' sub-folder. Please note, in order to run the hardware-side of the analysis, a valid IBM platform token and instance token are needed. However, to run the fully-simulated data collection (fraction = 0.0), this is not needed. Please also note that the temp backup files are currently present; therefore, the script will not re-simulate and will instead run from those backups. To re-generate the data, please delete the respective cached temp folder.

# 'processing_script.py': 
MMD analysis script. Returns the MMD test statistic for the defined proportion, extracted from the JSON backup files ('Data/training_data/cached_data/temp_{fraction}'). To validate the test statistic, it also runs a permutation test (using the MMD as the test statistic) and a one-sided (asymptotic) p value, which provides a probability of observing a statistic at least as large as the observed value under the null. The general null hypothesis is that both data groups are retrieved from the same underlying distribution. Warning, due to the permutation test, this is quite a slow script (takes around an hour with 2000 permutations). Also, you need quite a bit of allocated memory (about 1.47 GB) to run the 14062 x 14062 pairwise kernel matrix the unbiased quadratic MMD statistic generates. 
It should be noted that the analysis found minimal divergence (MMD^2=0.00081, p<0.001), therefore you safely ignore this script.

# 'general_model_trainer.py': 
Verifies the feature engineering conducted in 'data_collector.py' by comparing the RMSE between the feature-engineered data and the raw data. If it is the same, then it's considered to have all the necessary information for error mitigation. It then has the hyperparameter tuning code (which is toggled off along with the other tuned models), and each, respective tuned model. This saves the model using a temp filename and calls the test code, which uses the saved model on the test data to assess predictive accuracy for a given combination of hyperparameters, as defined by a Latin hypercube. When in training mode, alongside verification print statements and average error print statements, it trains the model on the training data and saves the trained model to the 'Data/models' sub-folder for each respective experimental condition (proportion) and for each model.

# 'test_data_extractor.py': 
Collects the RB and mirror circuits from the IBM hardware. Please note, this script will not work without active platform and instance tokens. The script outputs the combined test data as csv files as either 'test_data_raw.csv' or 'test_data_raw_rt.csv' (for the retest data) in the 'Data/test_data' sub-folder.

#'mitigation_script.py': 
This script loads in the saved model and the raw test data ('test_data_raw.csv/rt') and then uses it to predict the results. The noiseless ideal vectors were also simulated from the AerSimulator to ensure that extrapolations were accurate. As the model's learned parameters correspond to the feature set and ordering used during training, the script also rebuilds that same feature structure from the training data to ensure predictions are applied to the correct inputs. The script returns the mitigated results as 'test_data_mitigated{%}_{model}.csv' file to the 'Data/mitigated_results' sub-folder.

# 'test_code.py': 
Loads raw test data and the mitigated results ('test_data_mitigated_{%}{model}.csv') and checks the RMSE for each of the test circuits. It returns an aggregate print statement and saves the individual test results as 'test_results{%}_{model}.csv' in 'Data/test_results'.

# 'analysis_suite.py': 
The main analysis suite. Loads in the test result CSVs ('test_results_{%}_{model}.csv') and runs the omnibus permutation test with a studentised max-t statistic, pairwise Wilcoxon signed-rank tests with Benjaminiâ€“Hochberg correction and bootstrap estimation of median paired differences with permuted p-values. Alongside printed summaries, it writes the results to 'analysis_results_all.csv' in the 'Data/analysis_results' folder.
